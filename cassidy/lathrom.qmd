---
title: "Ft Riley bumblebees"
number-sections: true
---

# Status of Native Bumblebees (Bombus spp.) at Fort Riley Military Reservation, Kansas

:::: {.columns}

::: {.column width='30%'}
![](studyarea.png)
:::

::: {.column width='70%'}
![](plot.png)
:::

::::
## Objective

- evaluate environmental factors affecting bumblebee density and abundance using distance sampling methods

:::{.callout-note collapse=false appearance='default' icon=true}
## Multiple years of data, my approach
There are three years of survey data. The first year (2022) was described as a pilot survey, so I will not thoroughly examine the 2022 data.
:::


# Exploratory data analysis for 2024


```{r}
#| label: tbl-readit
#| tbl-cap: "Approximate detections by species and bout for 2024"
library(readxl)
yr24 <- read_xlsx(path="data/2022-2024_bee_ds_data.xlsx", sheet = "2024")
(numstrat <- unique(yr24$Region.Label))
(numtrans <- unique(yr24$Sample.Label))
(numspec <- unique(yr24$species))
t24 <- table(yr24$species, yr24$sample.bout)
knitr::kable(t24)
```

Two things to note about the table above:
- many detections for some species and
- few detections for species I presume are rare (we'll deal with that shortly).

:::{.callout-important collapse=false appearance='default' icon=true}
## My presumption about objectives
- I'm guessing Cassidy would like estimates by species and by sampling bout
- We could easily ignore sampling bout and pool the data across bouts; this will produce density estimates that are the average density across the bouts. 
- Rather than pooling, I'll take the more difficult analytical approach of producing species- and bout-specific density estimates.
:::

Before starting the modelling, let's come to grips with the distribution of perpendicular detection distances for species and bout combinations.  This will give insight about what is in store for us when modelling begins.

```{r, histos}
#| layout-ncol: 2
histfn <- function(frame, spec) {
  hist(frame$distance[frame$species==spec & frame$sample.bout==1],
       main=paste(spec, " bout=May/June"), xlab="Distance", nc=20)
  hist(frame$distance[frame$species==spec & frame$sample.bout==2],
       main=paste(spec, " bout=July/August"), xlab="Distance", nc=20)
}
histfn(yr24, "AMER_BG")
histfn(yr24, "american")
histfn(yr24, "black_gold")
histfn(yr24, "brown_belted")
histfn(yr24, "carpenter")
histfn(yr24, "southern_plains")
```

Clearly there will be a struggle to fit detection function models to some of the species x bout combinations. Before we attack that problem, let's tackle a simpler problem: estimating density for the common "brown_belted".

# Brown belted analysis {#sec-brownbelt}

This species has nearly 200 detections for each sampling bout. Hence, if we wanted bout-specific estimates of density for this species, the most straightforward approach would be to simply pluck detections of `brown_belted` from the 2024 data frame.

```{r, bbsubset}
bb_bees_b1 <- subset(yr24, species=="brown_belted" & sample.bout==1)
bb_bees_b2 <- subset(yr24, species=="brown_belted" & sample.bout==2)
```

A quick check of the integrity of the survey design in the subsetted data  shows:

```{r, bbtransects}
print(length(unique(bb_bees_b1$Sample.Label)))
print(length(unique(bb_bees_b2$Sample.Label)))
```

Even this most common species is only seen on little more than one-third of the 151 transects surveyed. If we carried on like this, we would produce extremely positively biased estimates of density.

Solution, specify survey design manually using two arguments to the `ds` function: `region_table` and `sample_table`.

```{r, regsamtab}
bb_reg_tab1 <- data.frame(Region.Label="FRMR", Area=yr24$Area[1])
bb_reg_tab2 <- data.frame(Region.Label="FRMR", Area=yr24$Area[1])
bb_sam_tab1 <- data.frame(Sample.Label=seq(1, 151),
                         Region.Label=rep("FRMR",151),
                         Effort=rep(500,151))
bb_sam_tab2 <- data.frame(Sample.Label=seq(1, 151),
                         Region.Label=rep("FRMR",151),
                         Effort=rep(500,151))
```

## Data organisation complete, fit some models

A glance at the histograms produced earlier suggests an abrupt drop in detection probability around 5-8m, suggesting the flexibility of the hazard rate model will be useful, but we fit the full series of candidates to each bout. Note too the existence of a small number of detections at great distances, they will not be very useful in our detection function modelling. We are likely to be more aggressive in our truncation than we might otherwise be when dealing with non-insect species.

```{r, bbbout1}
#| message: false
#| warning: false
library(Distance)
cu <- convert_units("meter", "meter", "hectare")
bbunicos <- ds(data=bb_bees_b1, key="unif", adj="cos", convert_units = cu,
               region_table = bb_reg_tab1, sample_table = bb_sam_tab1,
               truncation = "10%")
bbhnherm <- ds(data=bb_bees_b1, key="hn", adj="herm", convert_units = cu,
               region_table = bb_reg_tab1, sample_table = bb_sam_tab1,
               truncation = "10%")
bbhrsim <- ds(data=bb_bees_b1, key="hr", adj="poly", convert_units = cu,
               region_table = bb_reg_tab1, sample_table = bb_sam_tab1,
               truncation = "10%")
knitr::kable(summarize_ds_models(bbunicos, bbhnherm, bbhrsim)[, c(2:7)],
             digits=3, row.names=FALSE,
             caption="Model criticism candidate model set\n brown_belted bout 1")
```

Double-check that we have the right number of transects:

```{r, numtran}
knitr::kable(bbunicos$dht$individuals$summary)
```

:::{.callout-note collapse=false appearance='default' icon=true}
## What did we learn from this?
- all models fit the data
- $\Delta$AIC among candidates is small
- Not by coincidence, $\hat{P}_a$ are very similar across models
:::

Before examining the density estimate, continue for sampling bout 2

```{r, bbbout2}
#| message: false
#| warning: false
library(Distance)
cu <- convert_units("meter", "meter", "hectare")
bbunicos2 <- ds(data=bb_bees_b2, key="unif", adj="cos", convert_units = cu,
               region_table = bb_reg_tab2, sample_table = bb_sam_tab2,
               truncation = "10%")
bbhnherm2 <- ds(data=bb_bees_b2, key="hn", adj="herm", convert_units = cu,
               region_table = bb_reg_tab2, sample_table = bb_sam_tab2,
               truncation = "10%")
bbhrsim2 <- ds(data=bb_bees_b2, key="hr", adj="poly", convert_units = cu,
               region_table = bb_reg_tab2, sample_table = bb_sam_tab2,
               truncation = "10%")
knitr::kable(summarize_ds_models(bbunicos2, bbhnherm2, bbhrsim2)[, c(2:7)],
             digits=3, row.names=FALSE,
             caption="Model criticism candidate model set\n brown_belted bout 2")
```

The difference in $\Delta$AIC among candidates is greater for bout 2, and you will see a bit more disparity between $\hat{P}_a$, particularly the non-fitting half normal model.

Let's look at the fit of the selected models for each bout:

```{r, bbplots}
#| layout-ncol: 2
plot(bbunicos, nc=20, main="brown_belted, bout 1, unifcos")
plot(bbhrsim2, nc=20, main="brown_belted, bout 2, hazard rate")
```

Looking at the plots, I think I would truncate yet more aggressively again; to ~30m.

```{r moretrunc}
#| message: false
#| warning: false
bbunicosx <- ds(data=bb_bees_b1, key="unif", adj="cos", convert_units = cu,
               region_table = bb_reg_tab1, sample_table = bb_sam_tab1,
               truncation = 31)
bbhnhermx <- ds(data=bb_bees_b1, key="hn", adj="herm", convert_units = cu,
               region_table = bb_reg_tab1, sample_table = bb_sam_tab1,
               truncation = 31)
bbhrsimx <- ds(data=bb_bees_b1, key="hr", adj="poly", convert_units = cu,
               region_table = bb_reg_tab1, sample_table = bb_sam_tab1,
               truncation = 31)
bbunicos2x <- ds(data=bb_bees_b2, key="unif", adj="cos", convert_units = cu,
               region_table = bb_reg_tab2, sample_table = bb_sam_tab2,
               truncation = 31)
bbhnherm2x <- ds(data=bb_bees_b2, key="hn", adj="herm", convert_units = cu,
               region_table = bb_reg_tab2, sample_table = bb_sam_tab2,
               truncation = 31)
bbhrsim2x <- ds(data=bb_bees_b2, key="hr", adj="poly", convert_units = cu,
               region_table = bb_reg_tab2, sample_table = bb_sam_tab2,
               truncation = 31)
knitr::kable(summarize_ds_models(bbunicosx, bbhnhermx, bbhrsimx)[, c(2:7)],
             digits=3, row.names=FALSE,
             caption="Model criticism candidate model set\n brown_belted bout 1 trunc=31")
knitr::kable(summarize_ds_models(bbunicos2x, bbhnherm2x, bbhrsim2x)[, c(2:7)],
             digits=3, row.names=FALSE,
             caption="Model criticism candidate model set\n brown_belted bout 2 trunc=31")
```

```{r, bbplots30}
#| layout-ncol: 2
plot(bbunicosx, nc=20, main="brown_belted, bout 1, unifcos, trunc=31")
plot(bbhrsim2x, nc=20, main="brown_belted, bout 2, hazard rate, trunc=31")
```

## Did truncation changes influence estimated density?

We have seen there is no change in the models selected when changing the truncation distance, how about the estimates?


```{r, bbestimates}
bb1_10 <- bbunicos$dht$individuals$D
bb2_10 <- bbhrsim$dht$individuals$D
bb1_31 <- bbunicosx$dht$individuals$D
bb2_31 <- bbhrsimx$dht$individuals$D
estimates <- rbind(bb1_10, bb1_31, bb2_10, bb2_31)
estimates$Label <- c("Bout1 10%", "Bout1 31m", "Bout2 10%", "Bout2 31m")
knitr::kable(estimates, digits=3,
             caption="Estimates for brown belted, both bouts, different truncation distances")
```

We see roughly a 3-5\% difference in the point estimates from the more aggressive truncation. Place that difference in the context of the coefficient of variation (~20\%).

<!--
Reorganise the data columns just a bit

```{r}
#| eval: false
yr22$Sample.Label...12 <- NULL
yr22$Sample.Label <- yr22$Sample.Label...5
yr22$Sample.Label...5 <- NULL
yr22 <- yr22[yr22$distance>=0,]  # a couple of -1 values
(numstrat <- unique(yr22$Region.Label))
(numtrans <- unique(yr22$Sample.Label))
(numspec <- unique(yr22$species))
```


```{r}
#| eval: false
t22 <- table(yr22$species, yr22$sample.bout)
t23 <- table(yr23$species, yr23$sample.bout)
t24 <- table(yr24$species, yr24$sample.bout)
```

```{r}
#| eval: false
#| layout-ncol: 2
histfn <- function(frame, spec, bout) {
  hist(frame$distance[frame$species==spec & frame$sample.bout==bout],
       main=paste(spec, " bout=", bout), xlab="Distance")
}
histfn(yr22, "bombus", 1)
histfn(yr22, "bombus", 2)
histfn(yr22, "carpenter", 1)
histfn(yr22, "carpenter", 2)
histfn(yr22, "unknown", 1)
histfn(yr22, "unknown", 2)
```

-->

We could continue to carry out this species by species, bout by bout analysis for the common species. However, eventually problems will arise when get to species like `AMER_BG`, `black_gold`, `bombus`, etc. If estimates for those species x bout combinations are important, we will have to "borrow strength" from other species to produce robust estimates for those rare species.

We "borrow strength" via the use of species and sampling bout as covariates in the detection function. I ignore other potential covariates, e.g. observer, because the property of pooling robustness ensures that bias is not introduced by ignoring such covariates.

# Use 2024 to build 2-level stratification

Note, for this current analysis, I carry the `twsp` and `unk` species in the analysis; in all likelihood, those species should probably be deleted before the analysis continues as robust estimates for those species x bout combination are unlikely to be informative.

:::{.callout-warning collapse=false appearance='default' icon=true}
## But make sure design (effort and transects) are respected
- There should be 151 transects each 500m in length for each of our "strata"
- even though strata are not geographic, but rather species and bout defined
:::

## Construct region table and sample tables

:::{.callout-warning collapse=false appearance='default' icon=true}
## The following step is **critical**
- If the following specification of the survey design is not followed, the analysis will issue no complaints and will produce density estimates that appear to be quite wonderful.
- However, they will be wrong.
- Because the stratum redefinition destroys the information about the survey design.
- Instead, the software assumes there were only transects on which detections of the species x bout combinations were made.
- Always check the summary output from your fitted models to make sure the correct number of transects are represented.
:::


```{r, redefinestrata}
yr24$Region.Label <- paste0(yr24$species, yr24$sample.bout)
new_region_table <- data.frame(Region.Label=unique(yr24$Region.Label),
                               Area=28383)
new_sample_table <- data.frame(Sample.Label=rep(seq(1,151), 19),
                               Region.Label=rep(unique(yr24$Region.Label), 151),
                               Effort=rep(500, 19*151))
```


```{r, bigmodelsuite}
#| message: false
#| warning: false
yr24$Region.Label <- paste0(yr24$species, yr24$sample.bout)
yr24_hn_2level <- ds(yr24, key="hn", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table,
                     formula=~species+as.factor(sample.bout))
yr24_hr_2level <- ds(yr24, key="hr", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table,
                     formula=~species+as.factor(sample.bout))
yr24_hn_species <- ds(yr24, key="hn", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table,
                     formula=~species)
yr24_hr_species <- ds(yr24, key="hr", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table,
                     formula=~species)
yr24_hn_bout <- ds(yr24, key="hn", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table,
                     formula=~as.factor(sample.bout))
yr24_hr_bout <- ds(yr24, key="hr", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table,
                     formula=~as.factor(sample.bout))
yr24_hn <- ds(yr24, key="hn", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table)
yr24_hr <- ds(yr24, key="hr", truncation = "10%", convert_units = cu,
                     region_table = new_region_table,
                     sample_table = new_sample_table)
knitr::kable(summarize_ds_models(#yr24_hn, yr24_hr, 
                    yr24_hn_bout, yr24_hr_bout, 
                    yr24_hn_species, yr24_hr_species, 
                    yr24_hn_2level, yr24_hr_2level)[, c(2:7)],
             digits=3, row.names=FALSE,
             caption="Model criticism candidate model set 2024")

knitr::kable(summary(yr24_hr_2level$ddf)$coeff$key.scale, 
             caption="Looking for convergence problems in covariate coefficients")
```

## Given the preferred model is OK, look at the plot

```{r, complexfig}
#| fig.height: 7
#| fig.width: 9
plot(yr24_hr_2level, nc=39)
sp <- unique(yr24$species)
bo <- unique(yr24$sample.bout)
palette("ggplot2")
for (i in 1:length(sp)) {
  for (j in 1:length(bo))
  add_df_covar_line(yr24_hr_2level, 
                    data=data.frame(species=sp[i], sample.bout=bo[j]),
                    lwd=2, lty=1, col=rainbow(19)[i])
}
legend("topright", title="Spec x bout", legend=unique(yr24$Region.Label),
       lwd=2, lty=1, col=rainbow(19), cex=0.8)
```

:::{.callout-note collapse=false appearance='default' icon=true}
## Species and bout-specific detection probability estimates
Although the figure is pretty, we cannot deduce the species x bout-specific detection probabilities. Assuming these estimates are of interest, how do we produce them, given the software won't cooperate with our request.  Answer integrate detection function over distance for specific covariate levels.
:::

Below is a function that will calculate detection probability for a given set of estimated covariate coefficients at a given perpendicular distance. We will use the function, in conjunction with the coefficients for a species and bout combination of interest.

```{r, detfnfn}
gz<-function(z,
             beta, sigintercept, sigcoef, DistWin=FALSE,
             key="HR", w=max(z)){
#this is a generic detection function that returns the probability of detecting an animal
#    z               generic distance (perpendicular) - can be scalar or vector
#    beta            shape coefficient
#    sigintercept  intercept coefficient for sigma
#    sigcoef         coefficient for specific factor level
#    DistWin         coefficients from Distance for Windows or from R
#    key             the detection function key, works for hazard rate and half normal
#    w               truncation distance, by default the max of the distances
#
#RETURNS: a probability
  
  if(key != "HN" & key != "HR") {
    stop("Argument 'key' must be either HN or HR")
  }
  if (DistWin) {
    sigma <- sigintercept + exp(sigcoef)
    exponent <- beta
  } else {
    numterms <- length(sigcoef)
    predictor <- 0
    for (i in 1:numterms) {
      predictor <- predictor + sigcoef[i]
    }
    sigma <- exp(sigintercept + predictor)
    exponent <- exp(beta)
  }
  if(key=="HR") {
    scale.dist <- z/sigma
    inside <- -(scale.dist)^(-exponent)
    gx <- 1 - exp(inside)
  } else {
    scale.dist <- z  # debatably don't scale for half normal
    inside <- -(scale.dist^2/(2*sigma^2))
    gx <- exp(inside)
  }
  return(gx)
}
```

## Apply function to the selected `yr24_hr_2level` model

Code below plucks out the relevant estimated coefficients from our model that included species and bout as covariates. I demonstrate only for two species and the two bouts. Code then plots the derived detection function from the fitted covariate coefficients. This gives us a sense check whether we've picked up the coefficients correctly.

```{r, specboutdetfn}
coefs <- yr24_hr_2level$ddf$par
xmax <- unname(yr24_hr_2level$ddf$meta.data$width)
xvals <- seq(0,xmax, length=100)
am1 <- gz(z=xvals, key="HR",beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
          sigcoef = c(coefs["speciesamerican"]))
am2 <- gz(z=xvals, key="HR",beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
          sigcoef = c(coefs["speciesamerican"], coefs["as.factor.sample.bout.2"]))
bomb1 <- gz(z=xvals, key="HR",beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
          sigcoef = c(coefs["speciesbombus"]))
bomb2 <- gz(z=xvals, key="HR",beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
          sigcoef = c(coefs["speciesbombus"], coefs["as.factor.sample.bout.2"]))
plot(xvals, am1, lwd=2, type="l",
     main="Construct species x bout detection functions manually",
     xlab="Distance", ylab="Detection probability")
lines(xvals, am2, lwd=2, col="blue")
lines(xvals, bomb1, lwd=2, col="darkgreen")
lines(xvals, bomb2, lwd=2, col="red")
legend("topright", legend=c("American1", "American2", "Bombus1", "Bombus2"),
       col=c("black","blue","darkgreen","red"), lwd=2)
```

## Integration and conversion to detection probability

Sense check completed, we now use our `gz` function to "manually" compute species- and bout-specific detection probabilities for the for examples created above. The formula being applied here (see Lecture 2 of notes) is $\hat{P}_a$ is "area under curve" divided by "area of rectangle"

$$\hat{P}_a=\frac{\int^w_0 \hat{g}(x) dx}{w} $$

```{r, integration}
am1_p_int <- integrate(gz, lower=0, upper=xmax, key="HR",
                    beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
                    sigcoef = c(coefs["speciesamerican"]))$value / xmax
am2_p_int <- integrate(gz, lower=0, upper=xmax, key="HR",
                    beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
                    sigcoef = c(coefs["speciesamerican"], coefs["as.factor.sample.bout.2"]))$value / xmax
bomb1_p_int <- integrate(gz, lower=0, upper=xmax, key="HR",
                    beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
                    sigcoef = c(coefs["speciesbombus"]))$value / xmax
bomb2_p_int <- integrate(gz, lower=0, upper=xmax, key="HR",
                    beta=coefs["V1"], sigintercept = coefs["X.Intercept."], 
                    sigcoef = c(coefs["speciesbombus"], coefs["as.factor.sample.bout.2"]))$value / xmax
```

## Do the integrated Pa correspond to reported densities?

Recall our formula to estimate density:

$$\hat{D} = \frac{\hat{P}_a \cdot n}{a}$$
where $n$ is number of detections and $a$ is covered area.  Both of these quantities appear in the fitted model object, along with $\hat{D}$ for each species x bout combination.

We can rearrange the above formula to solve for $\hat{P}_a$:

$$\hat{P}_a = \frac{n}{\hat{D} \cdot a}$$
The following code chunk carries out this method of deriving $\hat{P}_a$ and compares them with the original method of computing the integral of the detection function and dividing by truncation distance, $w$.
```{r, allequal}
summ <- yr24_hr_2level$dht$individuals$summary
coveredarea <- yr24_hr_2level$dht$individuals$summary$CoveredArea[1]
d_est <- yr24_hr_2level$dht$individuals$D

am1_p_data <- summ[summ$Region=="american1", "n"] / (d_est[d_est$Label=="american1", "Estimate"] * coveredarea)
am2_p_data <- summ[summ$Region=="american2", "n"] / (d_est[d_est$Label=="american2", "Estimate"] * coveredarea)
bomb1_p_data <- summ[summ$Region=="bombus1", "n"] / (d_est[d_est$Label=="bombus1", "Estimate"] * coveredarea)
bomb2_p_data <- summ[summ$Region=="bombus2", "n"] / (d_est[d_est$Label=="bombus2", "Estimate"] * coveredarea)

all.equal(am1_p_int, am1_p_data)
all.equal(am2_p_int, am2_p_data)
all.equal(bomb1_p_int, bomb1_p_data)
all.equal(bomb2_p_int, bomb2_p_data)
```

Yes, the integration results in a $\hat{P}_a$ equivalent to that used in the estimates produced by the software (as if there was any doubt).

Now that the detection probability estimate dilemma is sorted, let's look at the objects of ecological interest, namely the species- and bout-specific density estimates.

# Density estimates from the hazard rate two-level stratification model

```{r}
#| label: tbl-twolevel
knitr::kable(yr24_hr_2level$dht$individuals$D[,1:6], digits=3, 
             caption="Density estimates when using hazard with species and bout covariates.")
```

## Compare estimates from "basic" analysis

Remember we started the modelling of the 2024 data by looking at the "brown_belted" species in Section 
@sec-brownbelt. In that analysis, each sampling bout for that species was modelled in isolation, as there were sufficient numbers of detections of that species in each bout.

After choosing the most appropriate models from that pair of analyses, density estimates were

```{r}
knitr::kable(estimates[c(1,3),], digits=2, caption="Estimated brown belted densities from 'standalone' analysis")
```

Compare these point and interval estimates to those generated from our 2-level stratification model:

```{r}
bb1cov <- yr24_hr_2level$dht$individuals$D[yr24_hr_2level$dht$individuals$D$Label=="brown_belted1", ]
bb2cov <- yr24_hr_2level$dht$individuals$D[yr24_hr_2level$dht$individuals$D$Label=="brown_belted2", ]
knitr::kable(rbind(bb1cov, bb2cov)[,1:6], digits=3,
             caption="Density estimates brown belted from HR covariate model")
```

There is some difference in the point estimate for sampling bout 2; however the confidence intervals for the estimates from the two modelling approaches are quite similar. With reasonable data, which these are, the estimates are often robust to decisions made during analysis.

# A plot of species- and bout-specific density estimates

Our estimates live in a concise table @tbl-twolevel produced by our 2-level stratification model. Perhaps we wish, in addition, to generate a plot of the density estimates and their uncertainty.  That is accomplished by the following code chunk. Note I have chosen not to include the `twsp` or `unk` species from the plot.

```{r}
simplename <- yr24_hr_2level$dht$individuals$D
simplename$index <- 1:dim(simplename)[1]
yrange <- range(c(simplename$lcl, simplename$ucl))
plot(simplename$index[1:16], simplename$Estimate[1:16], 
     ylim=c(0, yrange[2]), xaxt="n", xlab="Species x bout", ylab="Density (per hectare)",
     main="Bee estimates 2024 from covariate model\nwith species and sampling bout")
s <- seq(16)
segments(simplename$index[s], simplename$lcl[s], simplename$index[s], simplename$ucl[s])
axis(1, at=1:16, labels=simplename$Label[1:16], cex.axis=0.6, las=2)
```

## Magical estimates for rare species?

Density estimates are produced for all species in the data frame @tbl-readit; even for species with as few as four detections (eastern2). Is that some kind of magic? No, look closely at the measures of precision associated with the species with small numbers of detection:

```{r, rare_ests}
library(kableExtra)
knitr::kable(table.attr="quarto-disable-processing=true",
             format="html",
             yr24_hr_2level$dht$individuals$D[, 1:6],
             row.names = FALSE, digits=3) %>%
  row_spec(c(5:8,13:14,17), background = "salmon") %>%
  row_spec(c(18:20), strikeout=TRUE) %>%
  column_spec(c(2:6), width="4em")
```


Even though density estimates were produces for these species x bout combinations, their precision would suggest the estimates are not terribly reliable. It is comforting that the imprecision of these estimates is consistent with the small number of detections, even if there is some "borrowing of strength" from other species. This use of covariates may help in the modelling of the detection function, but remember the role that encounter rates and their variability plays in density estimation.

# Summary

This summarises the fundamental steps in analysing the 2024 Ft Riley bee survey data. The approach to take to the 2023 data would follow a similar path. If estimates from the 2022 data were of interest, it would be done in a similar fashion, although I note there were only two identified species and a large number of detections of unidentified bees.